<p align="center">
  <img src="https://bellmark.ai/brand-assets/svg/full-logo-dark.svg" alt="BeLLMark" width="320">
</p>

<h3 align="center">The Gold Standard for Self-Hosted LLM Evaluation</h3>

<p align="center">
  <a href="https://bellmark.ai">Website</a> · <a href="mailto:support@bellmark.ai">Contact</a>
</p>

---

## What is BeLLMark?

BeLLMark is a self-hosted LLM benchmarking studio for comparing language models with blind A/B/C evaluation, configurable LLM-as-judge scoring, and stakeholder-ready exports.

### Key Features

- **Blind A/B/C Evaluation** — Compare 1–6 models simultaneously with randomised presentation order
- **Configurable LLM Judges** — Use any model as a judge with custom criteria and scoring rubrics
- **Statistical Analysis** — Confidence intervals, effect sizes, significance tests, and bias detection
- **ELO Rating System** — Track model performance over time with a global leaderboard
- **Professional Exports** — HTML, JSON, CSV, PPTX, and PDF reports ready for stakeholders
- **Fully Self-Hosted** — Your data never leaves your machine

### Supported Providers

OpenAI · Anthropic · Google · Mistral · Groq · Together AI · Fireworks · DeepSeek · xAI · Cerebras · OpenRouter · LM Studio · Ollama · Any OpenAI-compatible endpoint

---

## Coming Soon

> **Source code will be available here upon launch.**
>
> Visit [bellmark.ai](https://bellmark.ai) to learn more and stay updated.

---

## License

License details will be published alongside the source code.

---

<p align="center">
  <sub>Built by <a href="https://github.com/Context-Management">Context Management</a></sub>
</p>
